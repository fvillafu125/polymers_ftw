{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4685fb70-7a05-4e31-977a-b88034e23e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lambda/nfs/FJV-Training/python_envs/polymer_comp/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from peft import PeftModel\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d888e3-4b12-4269-89ad-8ad1406206b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ids is (3,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ckpt/pretrain.pt and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'PolymerSmilesTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Paths to to best model checkpoints, loading of tokenizer, base_model, and test\n",
    "# csv file\n",
    "\n",
    "# --- Checkpoint, Test File, and Config Paths ---\n",
    "Tg_checkpoint_path = \"ckpt/neurips.pt/Tg\"\n",
    "FFV_checkpoint_path = \"ckpt/neurips.pt/FFV\"\n",
    "Tc_checkpoint_path = \"ckpt/neurips.pt/Tc\"\n",
    "density_checkpoint_path = \"ckpt/neurips.pt/density\"\n",
    "Rg_checkpoint_path = \"ckpt/neurips.pt/Rg\"\n",
    "\n",
    "checkpoints = [Tg_checkpoint_path, FFV_checkpoint_path, Tc_checkpoint_path,\n",
    "               density_checkpoint_path, Rg_checkpoint_path]\n",
    "test_csv = \"data/neurips-open-polymer-prediction-2025/test.csv\"\n",
    "finetune_config = yaml.load(open(\"config_finetune.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "# --- General Configuration ---\n",
    "num_properties = 1  # Set to number of regression targets\n",
    "blocksize = 411     # Set to match training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load Test File  ---\n",
    "df = pd.read_csv(test_csv)\n",
    "smiles_list = df[\"SMILES\"].tolist()\n",
    "ids = df[\"id\"].tolist()\n",
    "print(f\"Shape of ids is {np.shape(ids)}\")\n",
    "\n",
    "# --- Load Model & Tokenizer ---\n",
    "base_model = RobertaForSequenceClassification.from_pretrained(\n",
    "    finetune_config['model_path'],\n",
    "    num_labels=num_properties,\n",
    "    problem_type=\"regression\"\n",
    ")\n",
    "\n",
    "tokenizer = PolymerSmilesTokenizer.from_pretrained(\"roberta-base\", max_len=blocksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9825a938-77b7-4be3-8422-e0efb6465e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1109053969]\n",
      " [1422188626]\n",
      " [2032016830]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lambda/nfs/FJV-Training/python_envs/polymer_comp/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162.57144      0.3814352    0.22103797   1.1560313   23.261528  ]\n",
      " [191.25104      0.38276163   0.26801115   0.98656523  21.147026  ]\n",
      " [ 88.973175     0.36296433   0.27237323   1.1232697   18.875105  ]]\n"
     ]
    }
   ],
   "source": [
    "# Looping through inference with each checkpoint, and writing the results\n",
    "# to the submission file\n",
    "\n",
    "ID = [ids]\n",
    "ID = np.array(ID).astype(int)\n",
    "ID = np.transpose(ID).astype(int)\n",
    "print(ID)\n",
    "results = []\n",
    "\n",
    "for item in checkpoints:\n",
    "\n",
    "    loopresults = []\n",
    "    # --- Load Checkpoint Models ---\n",
    "    model = PeftModel.from_pretrained(base_model, item)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # --- Inference ---\n",
    "    with torch.no_grad():\n",
    "        for idx, smiles in zip(ids, smiles_list):\n",
    "            encoding = tokenizer(\n",
    "                str(smiles),\n",
    "                add_special_tokens=True,\n",
    "                max_length=blocksize,\n",
    "                return_token_type_ids=False,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids = encoding[\"input_ids\"].to(device)\n",
    "            attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = outputs.logits.squeeze().cpu().numpy()\n",
    "            # print(f\"predictions are {preds}\")\n",
    "\n",
    "            if item == Tg_checkpoint_path:\n",
    "\n",
    "                max_temp = 745.4\n",
    "                preds = preds*max_temp-273.15\n",
    "                # mean = 375.7287743413897\n",
    "                # std = 111.55579067640443\n",
    "                # preds = preds*std+mean-273.15\n",
    "            \n",
    "            loopresults.append(preds)\n",
    "    \n",
    "    results.append(loopresults)\n",
    "\n",
    "results = np.transpose(results)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcaf6d2c-32b2-45b9-9dd2-01b2295aa91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Results saved to submission1.csv.\n"
     ]
    }
   ],
   "source": [
    "# --- Save Results ---\n",
    "   \n",
    "results_df = pd.DataFrame(results, columns=[\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"])\n",
    "ids_df = pd.DataFrame(ID, columns=[\"id\"])\n",
    "final_results_df = pd.concat([ids_df, results_df],axis=1)\n",
    "final_results_df.to_csv(\"submission1.csv\", index=False)\n",
    "print(\"Inference complete. Results saved to submission1.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5697c-50ec-4318-8715-4be922383549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (polymer_comp)",
   "language": "python",
   "name": "polymer_comp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
