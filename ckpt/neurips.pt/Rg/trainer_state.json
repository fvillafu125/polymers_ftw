{
  "best_global_step": 13743,
  "best_metric": 3.9255282878875732,
  "best_model_checkpoint": "ckpt/neurips.pt/Rg/checkpoint-13743",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 15270,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03274394237066143,
      "grad_norm": 251.73463439941406,
      "learning_rate": 4.983955468238376e-05,
      "loss": 203.1397,
      "step": 50
    },
    {
      "epoch": 0.06548788474132286,
      "grad_norm": 393.8805847167969,
      "learning_rate": 4.9675834970530455e-05,
      "loss": 59.947,
      "step": 100
    },
    {
      "epoch": 0.09823182711198428,
      "grad_norm": 128.3095245361328,
      "learning_rate": 4.951211525867715e-05,
      "loss": 23.8644,
      "step": 150
    },
    {
      "epoch": 0.13097576948264572,
      "grad_norm": 219.18431091308594,
      "learning_rate": 4.934839554682384e-05,
      "loss": 20.0055,
      "step": 200
    },
    {
      "epoch": 0.16371971185330714,
      "grad_norm": 45.281036376953125,
      "learning_rate": 4.9184675834970536e-05,
      "loss": 20.6443,
      "step": 250
    },
    {
      "epoch": 0.19646365422396855,
      "grad_norm": 100.77205657958984,
      "learning_rate": 4.9020956123117225e-05,
      "loss": 17.6023,
      "step": 300
    },
    {
      "epoch": 0.22920759659463,
      "grad_norm": 171.4813995361328,
      "learning_rate": 4.885723641126392e-05,
      "loss": 16.1957,
      "step": 350
    },
    {
      "epoch": 0.26195153896529144,
      "grad_norm": 387.40789794921875,
      "learning_rate": 4.869351669941061e-05,
      "loss": 15.952,
      "step": 400
    },
    {
      "epoch": 0.29469548133595286,
      "grad_norm": 241.407958984375,
      "learning_rate": 4.8529796987557305e-05,
      "loss": 14.5536,
      "step": 450
    },
    {
      "epoch": 0.3274394237066143,
      "grad_norm": 726.3233032226562,
      "learning_rate": 4.8366077275703994e-05,
      "loss": 12.1852,
      "step": 500
    },
    {
      "epoch": 0.3601833660772757,
      "grad_norm": 90.77906036376953,
      "learning_rate": 4.820235756385069e-05,
      "loss": 12.3511,
      "step": 550
    },
    {
      "epoch": 0.3929273084479371,
      "grad_norm": 182.4771728515625,
      "learning_rate": 4.803863785199738e-05,
      "loss": 9.2808,
      "step": 600
    },
    {
      "epoch": 0.4256712508185986,
      "grad_norm": 222.35092163085938,
      "learning_rate": 4.7874918140144075e-05,
      "loss": 10.5056,
      "step": 650
    },
    {
      "epoch": 0.45841519318926,
      "grad_norm": 85.8408432006836,
      "learning_rate": 4.771119842829077e-05,
      "loss": 7.7936,
      "step": 700
    },
    {
      "epoch": 0.4911591355599214,
      "grad_norm": 118.33399200439453,
      "learning_rate": 4.754747871643746e-05,
      "loss": 7.0332,
      "step": 750
    },
    {
      "epoch": 0.5239030779305829,
      "grad_norm": 182.2769012451172,
      "learning_rate": 4.7383759004584156e-05,
      "loss": 7.5927,
      "step": 800
    },
    {
      "epoch": 0.5566470203012442,
      "grad_norm": 75.86054229736328,
      "learning_rate": 4.7220039292730845e-05,
      "loss": 6.0518,
      "step": 850
    },
    {
      "epoch": 0.5893909626719057,
      "grad_norm": 183.7230224609375,
      "learning_rate": 4.705631958087754e-05,
      "loss": 8.9452,
      "step": 900
    },
    {
      "epoch": 0.6221349050425671,
      "grad_norm": 225.4586944580078,
      "learning_rate": 4.6892599869024236e-05,
      "loss": 6.0482,
      "step": 950
    },
    {
      "epoch": 0.6548788474132285,
      "grad_norm": 141.3084716796875,
      "learning_rate": 4.6728880157170925e-05,
      "loss": 9.4805,
      "step": 1000
    },
    {
      "epoch": 0.68762278978389,
      "grad_norm": 261.1809997558594,
      "learning_rate": 4.656516044531762e-05,
      "loss": 6.0382,
      "step": 1050
    },
    {
      "epoch": 0.7203667321545514,
      "grad_norm": 72.29045104980469,
      "learning_rate": 4.640144073346431e-05,
      "loss": 7.3392,
      "step": 1100
    },
    {
      "epoch": 0.7531106745252129,
      "grad_norm": 315.4795837402344,
      "learning_rate": 4.6237721021611006e-05,
      "loss": 9.5404,
      "step": 1150
    },
    {
      "epoch": 0.7858546168958742,
      "grad_norm": 157.33815002441406,
      "learning_rate": 4.6074001309757695e-05,
      "loss": 5.768,
      "step": 1200
    },
    {
      "epoch": 0.8185985592665357,
      "grad_norm": 264.9054870605469,
      "learning_rate": 4.591028159790439e-05,
      "loss": 7.1757,
      "step": 1250
    },
    {
      "epoch": 0.8513425016371972,
      "grad_norm": 109.59071350097656,
      "learning_rate": 4.574656188605108e-05,
      "loss": 6.9473,
      "step": 1300
    },
    {
      "epoch": 0.8840864440078585,
      "grad_norm": 83.78961181640625,
      "learning_rate": 4.5582842174197775e-05,
      "loss": 8.94,
      "step": 1350
    },
    {
      "epoch": 0.91683038637852,
      "grad_norm": 78.03443908691406,
      "learning_rate": 4.541912246234447e-05,
      "loss": 4.6864,
      "step": 1400
    },
    {
      "epoch": 0.9495743287491814,
      "grad_norm": 54.475494384765625,
      "learning_rate": 4.525540275049116e-05,
      "loss": 5.0865,
      "step": 1450
    },
    {
      "epoch": 0.9823182711198428,
      "grad_norm": 275.76434326171875,
      "learning_rate": 4.5091683038637856e-05,
      "loss": 6.8213,
      "step": 1500
    },
    {
      "epoch": 1.0,
      "eval_loss": 6.165519714355469,
      "eval_runtime": 1.0905,
      "eval_samples_per_second": 564.861,
      "eval_steps_per_second": 70.608,
      "step": 1527
    },
    {
      "epoch": 1.0150622134905043,
      "grad_norm": 142.25363159179688,
      "learning_rate": 4.4927963326784545e-05,
      "loss": 7.5385,
      "step": 1550
    },
    {
      "epoch": 1.0478061558611658,
      "grad_norm": 66.96060180664062,
      "learning_rate": 4.476424361493124e-05,
      "loss": 5.4199,
      "step": 1600
    },
    {
      "epoch": 1.080550098231827,
      "grad_norm": 159.6167755126953,
      "learning_rate": 4.4600523903077936e-05,
      "loss": 5.7399,
      "step": 1650
    },
    {
      "epoch": 1.1132940406024885,
      "grad_norm": 91.09070587158203,
      "learning_rate": 4.4436804191224625e-05,
      "loss": 6.8116,
      "step": 1700
    },
    {
      "epoch": 1.14603798297315,
      "grad_norm": 184.26705932617188,
      "learning_rate": 4.427308447937132e-05,
      "loss": 5.9128,
      "step": 1750
    },
    {
      "epoch": 1.1787819253438114,
      "grad_norm": 188.30197143554688,
      "learning_rate": 4.410936476751801e-05,
      "loss": 6.5695,
      "step": 1800
    },
    {
      "epoch": 1.211525867714473,
      "grad_norm": 329.0069274902344,
      "learning_rate": 4.39456450556647e-05,
      "loss": 5.0858,
      "step": 1850
    },
    {
      "epoch": 1.2442698100851342,
      "grad_norm": 77.37450408935547,
      "learning_rate": 4.3781925343811395e-05,
      "loss": 5.5074,
      "step": 1900
    },
    {
      "epoch": 1.2770137524557956,
      "grad_norm": 109.43128204345703,
      "learning_rate": 4.361820563195809e-05,
      "loss": 5.6498,
      "step": 1950
    },
    {
      "epoch": 1.309757694826457,
      "grad_norm": 65.20024871826172,
      "learning_rate": 4.345448592010478e-05,
      "loss": 5.2496,
      "step": 2000
    },
    {
      "epoch": 1.3425016371971186,
      "grad_norm": 255.39772033691406,
      "learning_rate": 4.3290766208251476e-05,
      "loss": 5.4823,
      "step": 2050
    },
    {
      "epoch": 1.37524557956778,
      "grad_norm": 169.6753387451172,
      "learning_rate": 4.3127046496398165e-05,
      "loss": 5.7158,
      "step": 2100
    },
    {
      "epoch": 1.4079895219384415,
      "grad_norm": 138.35452270507812,
      "learning_rate": 4.296332678454486e-05,
      "loss": 6.2922,
      "step": 2150
    },
    {
      "epoch": 1.4407334643091028,
      "grad_norm": 40.94416046142578,
      "learning_rate": 4.2799607072691556e-05,
      "loss": 6.0981,
      "step": 2200
    },
    {
      "epoch": 1.4734774066797642,
      "grad_norm": 61.37701416015625,
      "learning_rate": 4.2635887360838245e-05,
      "loss": 5.2953,
      "step": 2250
    },
    {
      "epoch": 1.5062213490504257,
      "grad_norm": 202.41445922851562,
      "learning_rate": 4.247216764898494e-05,
      "loss": 7.3638,
      "step": 2300
    },
    {
      "epoch": 1.538965291421087,
      "grad_norm": 71.0513916015625,
      "learning_rate": 4.230844793713164e-05,
      "loss": 5.925,
      "step": 2350
    },
    {
      "epoch": 1.5717092337917484,
      "grad_norm": 70.330810546875,
      "learning_rate": 4.2144728225278326e-05,
      "loss": 5.9102,
      "step": 2400
    },
    {
      "epoch": 1.60445317616241,
      "grad_norm": 348.47674560546875,
      "learning_rate": 4.198100851342502e-05,
      "loss": 7.2732,
      "step": 2450
    },
    {
      "epoch": 1.6371971185330714,
      "grad_norm": 282.96929931640625,
      "learning_rate": 4.181728880157171e-05,
      "loss": 4.7254,
      "step": 2500
    },
    {
      "epoch": 1.6699410609037328,
      "grad_norm": 28.75263786315918,
      "learning_rate": 4.16535690897184e-05,
      "loss": 5.9791,
      "step": 2550
    },
    {
      "epoch": 1.7026850032743943,
      "grad_norm": 117.22825622558594,
      "learning_rate": 4.1489849377865095e-05,
      "loss": 3.3435,
      "step": 2600
    },
    {
      "epoch": 1.7354289456450558,
      "grad_norm": 269.023681640625,
      "learning_rate": 4.132612966601179e-05,
      "loss": 4.7616,
      "step": 2650
    },
    {
      "epoch": 1.768172888015717,
      "grad_norm": 35.496559143066406,
      "learning_rate": 4.116240995415848e-05,
      "loss": 5.9298,
      "step": 2700
    },
    {
      "epoch": 1.8009168303863785,
      "grad_norm": 40.519676208496094,
      "learning_rate": 4.0998690242305176e-05,
      "loss": 4.716,
      "step": 2750
    },
    {
      "epoch": 1.83366077275704,
      "grad_norm": 286.7740783691406,
      "learning_rate": 4.0834970530451865e-05,
      "loss": 5.2615,
      "step": 2800
    },
    {
      "epoch": 1.8664047151277012,
      "grad_norm": 84.81112670898438,
      "learning_rate": 4.067125081859856e-05,
      "loss": 4.7141,
      "step": 2850
    },
    {
      "epoch": 1.8991486574983627,
      "grad_norm": 173.91815185546875,
      "learning_rate": 4.0507531106745257e-05,
      "loss": 4.2555,
      "step": 2900
    },
    {
      "epoch": 1.9318925998690242,
      "grad_norm": 161.83053588867188,
      "learning_rate": 4.0343811394891946e-05,
      "loss": 6.6519,
      "step": 2950
    },
    {
      "epoch": 1.9646365422396856,
      "grad_norm": 464.90277099609375,
      "learning_rate": 4.018009168303864e-05,
      "loss": 5.3293,
      "step": 3000
    },
    {
      "epoch": 1.9973804846103471,
      "grad_norm": 123.51112365722656,
      "learning_rate": 4.001637197118534e-05,
      "loss": 6.0554,
      "step": 3050
    },
    {
      "epoch": 2.0,
      "eval_loss": 5.782018184661865,
      "eval_runtime": 1.1035,
      "eval_samples_per_second": 558.248,
      "eval_steps_per_second": 69.781,
      "step": 3054
    },
    {
      "epoch": 2.0301244269810086,
      "grad_norm": 120.11595916748047,
      "learning_rate": 3.9852652259332026e-05,
      "loss": 4.4099,
      "step": 3100
    },
    {
      "epoch": 2.06286836935167,
      "grad_norm": 24.021799087524414,
      "learning_rate": 3.968893254747872e-05,
      "loss": 6.3254,
      "step": 3150
    },
    {
      "epoch": 2.0956123117223315,
      "grad_norm": 35.11277770996094,
      "learning_rate": 3.952521283562541e-05,
      "loss": 4.5365,
      "step": 3200
    },
    {
      "epoch": 2.128356254092993,
      "grad_norm": 278.7855529785156,
      "learning_rate": 3.93614931237721e-05,
      "loss": 4.8325,
      "step": 3250
    },
    {
      "epoch": 2.161100196463654,
      "grad_norm": 90.96251678466797,
      "learning_rate": 3.9197773411918796e-05,
      "loss": 6.0443,
      "step": 3300
    },
    {
      "epoch": 2.1938441388343155,
      "grad_norm": 11.787123680114746,
      "learning_rate": 3.9034053700065485e-05,
      "loss": 3.327,
      "step": 3350
    },
    {
      "epoch": 2.226588081204977,
      "grad_norm": 288.0086669921875,
      "learning_rate": 3.887033398821218e-05,
      "loss": 7.092,
      "step": 3400
    },
    {
      "epoch": 2.2593320235756384,
      "grad_norm": 195.01800537109375,
      "learning_rate": 3.8706614276358876e-05,
      "loss": 5.0226,
      "step": 3450
    },
    {
      "epoch": 2.2920759659463,
      "grad_norm": 27.16790008544922,
      "learning_rate": 3.8542894564505565e-05,
      "loss": 5.9536,
      "step": 3500
    },
    {
      "epoch": 2.3248199083169614,
      "grad_norm": 99.89763641357422,
      "learning_rate": 3.837917485265226e-05,
      "loss": 4.796,
      "step": 3550
    },
    {
      "epoch": 2.357563850687623,
      "grad_norm": 92.4567642211914,
      "learning_rate": 3.821545514079896e-05,
      "loss": 4.2337,
      "step": 3600
    },
    {
      "epoch": 2.3903077930582843,
      "grad_norm": 88.12150573730469,
      "learning_rate": 3.8051735428945646e-05,
      "loss": 3.8876,
      "step": 3650
    },
    {
      "epoch": 2.423051735428946,
      "grad_norm": 295.5063171386719,
      "learning_rate": 3.788801571709234e-05,
      "loss": 3.9866,
      "step": 3700
    },
    {
      "epoch": 2.455795677799607,
      "grad_norm": 88.20481872558594,
      "learning_rate": 3.772429600523904e-05,
      "loss": 3.7739,
      "step": 3750
    },
    {
      "epoch": 2.4885396201702683,
      "grad_norm": 26.86806869506836,
      "learning_rate": 3.7560576293385726e-05,
      "loss": 4.7319,
      "step": 3800
    },
    {
      "epoch": 2.52128356254093,
      "grad_norm": 40.07606887817383,
      "learning_rate": 3.739685658153242e-05,
      "loss": 4.234,
      "step": 3850
    },
    {
      "epoch": 2.5540275049115913,
      "grad_norm": 182.36578369140625,
      "learning_rate": 3.723313686967911e-05,
      "loss": 4.9975,
      "step": 3900
    },
    {
      "epoch": 2.5867714472822527,
      "grad_norm": 120.85718536376953,
      "learning_rate": 3.70694171578258e-05,
      "loss": 6.1214,
      "step": 3950
    },
    {
      "epoch": 2.619515389652914,
      "grad_norm": 61.56061553955078,
      "learning_rate": 3.6905697445972496e-05,
      "loss": 4.2989,
      "step": 4000
    },
    {
      "epoch": 2.6522593320235757,
      "grad_norm": 290.1739196777344,
      "learning_rate": 3.6741977734119185e-05,
      "loss": 5.0063,
      "step": 4050
    },
    {
      "epoch": 2.685003274394237,
      "grad_norm": 333.6822204589844,
      "learning_rate": 3.657825802226588e-05,
      "loss": 4.4049,
      "step": 4100
    },
    {
      "epoch": 2.7177472167648986,
      "grad_norm": 140.45172119140625,
      "learning_rate": 3.6414538310412577e-05,
      "loss": 4.8418,
      "step": 4150
    },
    {
      "epoch": 2.75049115913556,
      "grad_norm": 42.66853332519531,
      "learning_rate": 3.6250818598559266e-05,
      "loss": 5.3347,
      "step": 4200
    },
    {
      "epoch": 2.7832351015062216,
      "grad_norm": 330.2279052734375,
      "learning_rate": 3.608709888670596e-05,
      "loss": 5.5974,
      "step": 4250
    },
    {
      "epoch": 2.815979043876883,
      "grad_norm": 25.404756546020508,
      "learning_rate": 3.592337917485266e-05,
      "loss": 3.329,
      "step": 4300
    },
    {
      "epoch": 2.848722986247544,
      "grad_norm": 666.2109375,
      "learning_rate": 3.5759659462999346e-05,
      "loss": 5.3948,
      "step": 4350
    },
    {
      "epoch": 2.8814669286182055,
      "grad_norm": 79.42261505126953,
      "learning_rate": 3.559593975114604e-05,
      "loss": 4.4549,
      "step": 4400
    },
    {
      "epoch": 2.914210870988867,
      "grad_norm": 262.8288269042969,
      "learning_rate": 3.543222003929274e-05,
      "loss": 4.2371,
      "step": 4450
    },
    {
      "epoch": 2.9469548133595285,
      "grad_norm": 58.38309097290039,
      "learning_rate": 3.526850032743943e-05,
      "loss": 6.3063,
      "step": 4500
    },
    {
      "epoch": 2.97969875573019,
      "grad_norm": 109.71217346191406,
      "learning_rate": 3.510478061558612e-05,
      "loss": 3.9906,
      "step": 4550
    },
    {
      "epoch": 3.0,
      "eval_loss": 5.031887531280518,
      "eval_runtime": 1.0843,
      "eval_samples_per_second": 568.131,
      "eval_steps_per_second": 71.016,
      "step": 4581
    },
    {
      "epoch": 3.0124426981008514,
      "grad_norm": 238.77664184570312,
      "learning_rate": 3.494106090373281e-05,
      "loss": 4.6925,
      "step": 4600
    },
    {
      "epoch": 3.045186640471513,
      "grad_norm": 63.430335998535156,
      "learning_rate": 3.47773411918795e-05,
      "loss": 6.4701,
      "step": 4650
    },
    {
      "epoch": 3.0779305828421744,
      "grad_norm": 150.78555297851562,
      "learning_rate": 3.4613621480026196e-05,
      "loss": 4.0546,
      "step": 4700
    },
    {
      "epoch": 3.110674525212836,
      "grad_norm": 485.1980285644531,
      "learning_rate": 3.4449901768172885e-05,
      "loss": 4.9335,
      "step": 4750
    },
    {
      "epoch": 3.143418467583497,
      "grad_norm": 40.266258239746094,
      "learning_rate": 3.428618205631958e-05,
      "loss": 3.2584,
      "step": 4800
    },
    {
      "epoch": 3.1761624099541583,
      "grad_norm": 269.9565734863281,
      "learning_rate": 3.412246234446628e-05,
      "loss": 4.1378,
      "step": 4850
    },
    {
      "epoch": 3.20890635232482,
      "grad_norm": 118.31291198730469,
      "learning_rate": 3.3958742632612966e-05,
      "loss": 3.8995,
      "step": 4900
    },
    {
      "epoch": 3.2416502946954813,
      "grad_norm": 21.1193790435791,
      "learning_rate": 3.379502292075966e-05,
      "loss": 4.1734,
      "step": 4950
    },
    {
      "epoch": 3.2743942370661427,
      "grad_norm": 42.84392547607422,
      "learning_rate": 3.363130320890636e-05,
      "loss": 3.4621,
      "step": 5000
    },
    {
      "epoch": 3.307138179436804,
      "grad_norm": 64.03510284423828,
      "learning_rate": 3.3467583497053046e-05,
      "loss": 4.6949,
      "step": 5050
    },
    {
      "epoch": 3.3398821218074657,
      "grad_norm": 81.37684631347656,
      "learning_rate": 3.330386378519974e-05,
      "loss": 4.0347,
      "step": 5100
    },
    {
      "epoch": 3.372626064178127,
      "grad_norm": 219.12625122070312,
      "learning_rate": 3.314014407334644e-05,
      "loss": 3.5746,
      "step": 5150
    },
    {
      "epoch": 3.4053700065487886,
      "grad_norm": 202.74916076660156,
      "learning_rate": 3.297642436149313e-05,
      "loss": 4.8052,
      "step": 5200
    },
    {
      "epoch": 3.43811394891945,
      "grad_norm": 67.84002685546875,
      "learning_rate": 3.2812704649639816e-05,
      "loss": 4.6904,
      "step": 5250
    },
    {
      "epoch": 3.4708578912901116,
      "grad_norm": 39.65099334716797,
      "learning_rate": 3.264898493778651e-05,
      "loss": 4.5357,
      "step": 5300
    },
    {
      "epoch": 3.5036018336607726,
      "grad_norm": 368.18023681640625,
      "learning_rate": 3.24852652259332e-05,
      "loss": 5.5627,
      "step": 5350
    },
    {
      "epoch": 3.536345776031434,
      "grad_norm": 130.1516876220703,
      "learning_rate": 3.2321545514079897e-05,
      "loss": 6.4491,
      "step": 5400
    },
    {
      "epoch": 3.5690897184020955,
      "grad_norm": 154.8575439453125,
      "learning_rate": 3.2157825802226586e-05,
      "loss": 4.1135,
      "step": 5450
    },
    {
      "epoch": 3.601833660772757,
      "grad_norm": 51.97938919067383,
      "learning_rate": 3.199410609037328e-05,
      "loss": 5.2111,
      "step": 5500
    },
    {
      "epoch": 3.6345776031434185,
      "grad_norm": 196.1188201904297,
      "learning_rate": 3.183038637851998e-05,
      "loss": 3.9566,
      "step": 5550
    },
    {
      "epoch": 3.66732154551408,
      "grad_norm": 359.87774658203125,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 3.5752,
      "step": 5600
    },
    {
      "epoch": 3.7000654878847414,
      "grad_norm": 34.54136276245117,
      "learning_rate": 3.150294695481336e-05,
      "loss": 5.8597,
      "step": 5650
    },
    {
      "epoch": 3.732809430255403,
      "grad_norm": 322.18389892578125,
      "learning_rate": 3.133922724296006e-05,
      "loss": 3.829,
      "step": 5700
    },
    {
      "epoch": 3.765553372626064,
      "grad_norm": 366.808349609375,
      "learning_rate": 3.117550753110675e-05,
      "loss": 3.4657,
      "step": 5750
    },
    {
      "epoch": 3.7982973149967254,
      "grad_norm": 196.61935424804688,
      "learning_rate": 3.101178781925344e-05,
      "loss": 3.5569,
      "step": 5800
    },
    {
      "epoch": 3.831041257367387,
      "grad_norm": 58.769676208496094,
      "learning_rate": 3.084806810740013e-05,
      "loss": 3.4764,
      "step": 5850
    },
    {
      "epoch": 3.8637851997380483,
      "grad_norm": 50.77073287963867,
      "learning_rate": 3.068434839554683e-05,
      "loss": 5.1408,
      "step": 5900
    },
    {
      "epoch": 3.89652914210871,
      "grad_norm": 51.96370315551758,
      "learning_rate": 3.0520628683693516e-05,
      "loss": 3.266,
      "step": 5950
    },
    {
      "epoch": 3.9292730844793713,
      "grad_norm": 20.528167724609375,
      "learning_rate": 3.035690897184021e-05,
      "loss": 3.4911,
      "step": 6000
    },
    {
      "epoch": 3.9620170268500328,
      "grad_norm": 142.45098876953125,
      "learning_rate": 3.0193189259986904e-05,
      "loss": 5.2884,
      "step": 6050
    },
    {
      "epoch": 3.9947609692206942,
      "grad_norm": 91.30894470214844,
      "learning_rate": 3.00294695481336e-05,
      "loss": 2.6817,
      "step": 6100
    },
    {
      "epoch": 4.0,
      "eval_loss": 4.329642295837402,
      "eval_runtime": 1.0802,
      "eval_samples_per_second": 570.286,
      "eval_steps_per_second": 71.286,
      "step": 6108
    },
    {
      "epoch": 4.027504911591356,
      "grad_norm": 259.66802978515625,
      "learning_rate": 2.986574983628029e-05,
      "loss": 4.2461,
      "step": 6150
    },
    {
      "epoch": 4.060248853962017,
      "grad_norm": 102.95035552978516,
      "learning_rate": 2.970203012442698e-05,
      "loss": 3.2353,
      "step": 6200
    },
    {
      "epoch": 4.092992796332679,
      "grad_norm": 31.941598892211914,
      "learning_rate": 2.9538310412573677e-05,
      "loss": 3.5823,
      "step": 6250
    },
    {
      "epoch": 4.12573673870334,
      "grad_norm": 158.70518493652344,
      "learning_rate": 2.9374590700720366e-05,
      "loss": 4.3803,
      "step": 6300
    },
    {
      "epoch": 4.158480681074002,
      "grad_norm": 134.0682830810547,
      "learning_rate": 2.9210870988867062e-05,
      "loss": 5.4365,
      "step": 6350
    },
    {
      "epoch": 4.191224623444663,
      "grad_norm": 33.74896240234375,
      "learning_rate": 2.9047151277013758e-05,
      "loss": 3.9931,
      "step": 6400
    },
    {
      "epoch": 4.2239685658153245,
      "grad_norm": 159.58633422851562,
      "learning_rate": 2.8883431565160447e-05,
      "loss": 4.272,
      "step": 6450
    },
    {
      "epoch": 4.256712508185986,
      "grad_norm": 449.7940673828125,
      "learning_rate": 2.871971185330714e-05,
      "loss": 6.1962,
      "step": 6500
    },
    {
      "epoch": 4.2894564505566475,
      "grad_norm": 115.53181457519531,
      "learning_rate": 2.855599214145383e-05,
      "loss": 4.6886,
      "step": 6550
    },
    {
      "epoch": 4.322200392927308,
      "grad_norm": 417.3365478515625,
      "learning_rate": 2.8392272429600524e-05,
      "loss": 3.4642,
      "step": 6600
    },
    {
      "epoch": 4.3549443352979695,
      "grad_norm": 261.69354248046875,
      "learning_rate": 2.822855271774722e-05,
      "loss": 3.0904,
      "step": 6650
    },
    {
      "epoch": 4.387688277668631,
      "grad_norm": 111.4833755493164,
      "learning_rate": 2.806483300589391e-05,
      "loss": 3.4556,
      "step": 6700
    },
    {
      "epoch": 4.4204322200392925,
      "grad_norm": 221.37545776367188,
      "learning_rate": 2.7901113294040605e-05,
      "loss": 3.773,
      "step": 6750
    },
    {
      "epoch": 4.453176162409954,
      "grad_norm": 85.30986022949219,
      "learning_rate": 2.7737393582187297e-05,
      "loss": 3.2101,
      "step": 6800
    },
    {
      "epoch": 4.485920104780615,
      "grad_norm": 87.60177612304688,
      "learning_rate": 2.757367387033399e-05,
      "loss": 6.5936,
      "step": 6850
    },
    {
      "epoch": 4.518664047151277,
      "grad_norm": 308.8306884765625,
      "learning_rate": 2.7409954158480682e-05,
      "loss": 4.1935,
      "step": 6900
    },
    {
      "epoch": 4.551407989521938,
      "grad_norm": 200.72158813476562,
      "learning_rate": 2.7246234446627378e-05,
      "loss": 2.9928,
      "step": 6950
    },
    {
      "epoch": 4.5841519318926,
      "grad_norm": 114.04735565185547,
      "learning_rate": 2.7082514734774067e-05,
      "loss": 3.2871,
      "step": 7000
    },
    {
      "epoch": 4.616895874263261,
      "grad_norm": 313.1051330566406,
      "learning_rate": 2.6918795022920763e-05,
      "loss": 4.5495,
      "step": 7050
    },
    {
      "epoch": 4.649639816633923,
      "grad_norm": 127.7087631225586,
      "learning_rate": 2.675507531106745e-05,
      "loss": 3.2194,
      "step": 7100
    },
    {
      "epoch": 4.682383759004584,
      "grad_norm": 164.75498962402344,
      "learning_rate": 2.6591355599214147e-05,
      "loss": 4.1267,
      "step": 7150
    },
    {
      "epoch": 4.715127701375246,
      "grad_norm": 82.92556762695312,
      "learning_rate": 2.642763588736084e-05,
      "loss": 4.2315,
      "step": 7200
    },
    {
      "epoch": 4.747871643745907,
      "grad_norm": 155.95066833496094,
      "learning_rate": 2.626391617550753e-05,
      "loss": 3.1301,
      "step": 7250
    },
    {
      "epoch": 4.780615586116569,
      "grad_norm": 53.256744384765625,
      "learning_rate": 2.6100196463654225e-05,
      "loss": 4.7772,
      "step": 7300
    },
    {
      "epoch": 4.81335952848723,
      "grad_norm": 47.36882400512695,
      "learning_rate": 2.593647675180092e-05,
      "loss": 3.8205,
      "step": 7350
    },
    {
      "epoch": 4.846103470857892,
      "grad_norm": 111.74056243896484,
      "learning_rate": 2.577275703994761e-05,
      "loss": 2.8287,
      "step": 7400
    },
    {
      "epoch": 4.878847413228553,
      "grad_norm": 15.505491256713867,
      "learning_rate": 2.5609037328094305e-05,
      "loss": 3.8563,
      "step": 7450
    },
    {
      "epoch": 4.911591355599214,
      "grad_norm": 37.01295852661133,
      "learning_rate": 2.5445317616240997e-05,
      "loss": 3.3204,
      "step": 7500
    },
    {
      "epoch": 4.944335297969875,
      "grad_norm": 155.05728149414062,
      "learning_rate": 2.528159790438769e-05,
      "loss": 3.8555,
      "step": 7550
    },
    {
      "epoch": 4.977079240340537,
      "grad_norm": 55.394309997558594,
      "learning_rate": 2.5117878192534382e-05,
      "loss": 4.4422,
      "step": 7600
    },
    {
      "epoch": 5.0,
      "eval_loss": 4.67208194732666,
      "eval_runtime": 1.0827,
      "eval_samples_per_second": 568.922,
      "eval_steps_per_second": 71.115,
      "step": 7635
    },
    {
      "epoch": 5.009823182711198,
      "grad_norm": 88.56322479248047,
      "learning_rate": 2.4954158480681075e-05,
      "loss": 3.3605,
      "step": 7650
    },
    {
      "epoch": 5.04256712508186,
      "grad_norm": 30.626922607421875,
      "learning_rate": 2.4790438768827767e-05,
      "loss": 4.1482,
      "step": 7700
    },
    {
      "epoch": 5.075311067452521,
      "grad_norm": 148.0190887451172,
      "learning_rate": 2.4626719056974463e-05,
      "loss": 4.5698,
      "step": 7750
    },
    {
      "epoch": 5.1080550098231825,
      "grad_norm": 82.37051391601562,
      "learning_rate": 2.4462999345121155e-05,
      "loss": 3.7986,
      "step": 7800
    },
    {
      "epoch": 5.140798952193844,
      "grad_norm": 123.22960662841797,
      "learning_rate": 2.4299279633267848e-05,
      "loss": 3.3689,
      "step": 7850
    },
    {
      "epoch": 5.1735428945645054,
      "grad_norm": 274.2393798828125,
      "learning_rate": 2.413555992141454e-05,
      "loss": 3.9324,
      "step": 7900
    },
    {
      "epoch": 5.206286836935167,
      "grad_norm": 52.972991943359375,
      "learning_rate": 2.3971840209561232e-05,
      "loss": 4.3839,
      "step": 7950
    },
    {
      "epoch": 5.239030779305828,
      "grad_norm": 17.355737686157227,
      "learning_rate": 2.3808120497707925e-05,
      "loss": 4.0358,
      "step": 8000
    },
    {
      "epoch": 5.27177472167649,
      "grad_norm": 511.2425231933594,
      "learning_rate": 2.3644400785854617e-05,
      "loss": 4.3454,
      "step": 8050
    },
    {
      "epoch": 5.304518664047151,
      "grad_norm": 73.70787811279297,
      "learning_rate": 2.3480681074001313e-05,
      "loss": 2.7935,
      "step": 8100
    },
    {
      "epoch": 5.337262606417813,
      "grad_norm": 51.94338607788086,
      "learning_rate": 2.3316961362148005e-05,
      "loss": 3.0698,
      "step": 8150
    },
    {
      "epoch": 5.370006548788474,
      "grad_norm": 138.37379455566406,
      "learning_rate": 2.3153241650294698e-05,
      "loss": 4.0268,
      "step": 8200
    },
    {
      "epoch": 5.402750491159136,
      "grad_norm": 127.74176788330078,
      "learning_rate": 2.2989521938441387e-05,
      "loss": 3.8963,
      "step": 8250
    },
    {
      "epoch": 5.435494433529797,
      "grad_norm": 72.44874572753906,
      "learning_rate": 2.2825802226588083e-05,
      "loss": 2.6788,
      "step": 8300
    },
    {
      "epoch": 5.468238375900459,
      "grad_norm": 109.8559799194336,
      "learning_rate": 2.2662082514734775e-05,
      "loss": 4.0273,
      "step": 8350
    },
    {
      "epoch": 5.50098231827112,
      "grad_norm": 114.94550323486328,
      "learning_rate": 2.2498362802881467e-05,
      "loss": 3.8686,
      "step": 8400
    },
    {
      "epoch": 5.533726260641782,
      "grad_norm": 223.71917724609375,
      "learning_rate": 2.233464309102816e-05,
      "loss": 2.73,
      "step": 8450
    },
    {
      "epoch": 5.566470203012443,
      "grad_norm": 129.29652404785156,
      "learning_rate": 2.2170923379174856e-05,
      "loss": 4.4301,
      "step": 8500
    },
    {
      "epoch": 5.599214145383105,
      "grad_norm": 93.67434692382812,
      "learning_rate": 2.2007203667321548e-05,
      "loss": 3.0333,
      "step": 8550
    },
    {
      "epoch": 5.631958087753766,
      "grad_norm": 69.63311004638672,
      "learning_rate": 2.1843483955468237e-05,
      "loss": 3.7834,
      "step": 8600
    },
    {
      "epoch": 5.664702030124427,
      "grad_norm": 54.628318786621094,
      "learning_rate": 2.1679764243614933e-05,
      "loss": 2.5458,
      "step": 8650
    },
    {
      "epoch": 5.697445972495088,
      "grad_norm": 67.0523452758789,
      "learning_rate": 2.1516044531761625e-05,
      "loss": 4.9068,
      "step": 8700
    },
    {
      "epoch": 5.73018991486575,
      "grad_norm": 49.93272018432617,
      "learning_rate": 2.1352324819908318e-05,
      "loss": 5.0657,
      "step": 8750
    },
    {
      "epoch": 5.762933857236411,
      "grad_norm": 97.58599090576172,
      "learning_rate": 2.118860510805501e-05,
      "loss": 3.3128,
      "step": 8800
    },
    {
      "epoch": 5.7956777996070725,
      "grad_norm": 68.776123046875,
      "learning_rate": 2.1024885396201706e-05,
      "loss": 4.4851,
      "step": 8850
    },
    {
      "epoch": 5.828421741977734,
      "grad_norm": 122.27388000488281,
      "learning_rate": 2.0861165684348398e-05,
      "loss": 4.7429,
      "step": 8900
    },
    {
      "epoch": 5.8611656843483955,
      "grad_norm": 101.12470245361328,
      "learning_rate": 2.0697445972495087e-05,
      "loss": 3.8848,
      "step": 8950
    },
    {
      "epoch": 5.893909626719057,
      "grad_norm": 68.54989624023438,
      "learning_rate": 2.0533726260641783e-05,
      "loss": 3.2864,
      "step": 9000
    },
    {
      "epoch": 5.926653569089718,
      "grad_norm": 48.11408615112305,
      "learning_rate": 2.0370006548788475e-05,
      "loss": 2.8679,
      "step": 9050
    },
    {
      "epoch": 5.95939751146038,
      "grad_norm": 66.37540435791016,
      "learning_rate": 2.0206286836935168e-05,
      "loss": 4.3584,
      "step": 9100
    },
    {
      "epoch": 5.992141453831041,
      "grad_norm": 157.6960906982422,
      "learning_rate": 2.004256712508186e-05,
      "loss": 3.1364,
      "step": 9150
    },
    {
      "epoch": 6.0,
      "eval_loss": 4.128420352935791,
      "eval_runtime": 1.0746,
      "eval_samples_per_second": 573.223,
      "eval_steps_per_second": 71.653,
      "step": 9162
    },
    {
      "epoch": 6.024885396201703,
      "grad_norm": 61.835575103759766,
      "learning_rate": 1.9878847413228556e-05,
      "loss": 3.5277,
      "step": 9200
    },
    {
      "epoch": 6.057629338572364,
      "grad_norm": 109.23651885986328,
      "learning_rate": 1.9715127701375248e-05,
      "loss": 2.7429,
      "step": 9250
    },
    {
      "epoch": 6.090373280943026,
      "grad_norm": 169.8711395263672,
      "learning_rate": 1.9551407989521937e-05,
      "loss": 2.6261,
      "step": 9300
    },
    {
      "epoch": 6.123117223313687,
      "grad_norm": 242.78807067871094,
      "learning_rate": 1.9387688277668633e-05,
      "loss": 2.2529,
      "step": 9350
    },
    {
      "epoch": 6.155861165684349,
      "grad_norm": 40.102272033691406,
      "learning_rate": 1.9223968565815325e-05,
      "loss": 4.0927,
      "step": 9400
    },
    {
      "epoch": 6.18860510805501,
      "grad_norm": 22.94954490661621,
      "learning_rate": 1.9060248853962018e-05,
      "loss": 3.6808,
      "step": 9450
    },
    {
      "epoch": 6.221349050425672,
      "grad_norm": 47.72297668457031,
      "learning_rate": 1.889652914210871e-05,
      "loss": 2.9224,
      "step": 9500
    },
    {
      "epoch": 6.254092992796332,
      "grad_norm": 206.57827758789062,
      "learning_rate": 1.8732809430255406e-05,
      "loss": 3.2731,
      "step": 9550
    },
    {
      "epoch": 6.286836935166994,
      "grad_norm": 59.596195220947266,
      "learning_rate": 1.85690897184021e-05,
      "loss": 2.9045,
      "step": 9600
    },
    {
      "epoch": 6.319580877537655,
      "grad_norm": 61.3123664855957,
      "learning_rate": 1.8405370006548787e-05,
      "loss": 2.5913,
      "step": 9650
    },
    {
      "epoch": 6.352324819908317,
      "grad_norm": 33.03810501098633,
      "learning_rate": 1.824165029469548e-05,
      "loss": 3.022,
      "step": 9700
    },
    {
      "epoch": 6.385068762278978,
      "grad_norm": 18.056243896484375,
      "learning_rate": 1.8077930582842176e-05,
      "loss": 3.1992,
      "step": 9750
    },
    {
      "epoch": 6.41781270464964,
      "grad_norm": 198.01564025878906,
      "learning_rate": 1.7914210870988868e-05,
      "loss": 3.6486,
      "step": 9800
    },
    {
      "epoch": 6.450556647020301,
      "grad_norm": 91.75120544433594,
      "learning_rate": 1.775049115913556e-05,
      "loss": 3.6782,
      "step": 9850
    },
    {
      "epoch": 6.4833005893909625,
      "grad_norm": 64.66777038574219,
      "learning_rate": 1.7586771447282256e-05,
      "loss": 4.4391,
      "step": 9900
    },
    {
      "epoch": 6.516044531761624,
      "grad_norm": 108.41607666015625,
      "learning_rate": 1.742305173542895e-05,
      "loss": 5.4141,
      "step": 9950
    },
    {
      "epoch": 6.5487884741322855,
      "grad_norm": 47.12295913696289,
      "learning_rate": 1.7259332023575638e-05,
      "loss": 4.1912,
      "step": 10000
    },
    {
      "epoch": 6.581532416502947,
      "grad_norm": 39.07290267944336,
      "learning_rate": 1.709561231172233e-05,
      "loss": 3.7716,
      "step": 10050
    },
    {
      "epoch": 6.614276358873608,
      "grad_norm": 325.7079162597656,
      "learning_rate": 1.6931892599869026e-05,
      "loss": 3.2658,
      "step": 10100
    },
    {
      "epoch": 6.64702030124427,
      "grad_norm": 20.983896255493164,
      "learning_rate": 1.6768172888015718e-05,
      "loss": 3.3702,
      "step": 10150
    },
    {
      "epoch": 6.679764243614931,
      "grad_norm": 59.730873107910156,
      "learning_rate": 1.660445317616241e-05,
      "loss": 3.064,
      "step": 10200
    },
    {
      "epoch": 6.712508185985593,
      "grad_norm": 61.32939529418945,
      "learning_rate": 1.6440733464309106e-05,
      "loss": 3.2545,
      "step": 10250
    },
    {
      "epoch": 6.745252128356254,
      "grad_norm": 59.66122055053711,
      "learning_rate": 1.6277013752455795e-05,
      "loss": 3.2907,
      "step": 10300
    },
    {
      "epoch": 6.777996070726916,
      "grad_norm": 438.1856689453125,
      "learning_rate": 1.6113294040602488e-05,
      "loss": 4.4312,
      "step": 10350
    },
    {
      "epoch": 6.810740013097577,
      "grad_norm": 52.65823745727539,
      "learning_rate": 1.594957432874918e-05,
      "loss": 4.8374,
      "step": 10400
    },
    {
      "epoch": 6.843483955468239,
      "grad_norm": 123.61455535888672,
      "learning_rate": 1.5785854616895876e-05,
      "loss": 5.0347,
      "step": 10450
    },
    {
      "epoch": 6.8762278978389,
      "grad_norm": 247.76336669921875,
      "learning_rate": 1.5622134905042568e-05,
      "loss": 2.8353,
      "step": 10500
    },
    {
      "epoch": 6.908971840209562,
      "grad_norm": 186.11708068847656,
      "learning_rate": 1.545841519318926e-05,
      "loss": 2.9642,
      "step": 10550
    },
    {
      "epoch": 6.941715782580223,
      "grad_norm": 67.64315032958984,
      "learning_rate": 1.5294695481335953e-05,
      "loss": 3.3867,
      "step": 10600
    },
    {
      "epoch": 6.974459724950884,
      "grad_norm": 99.21378326416016,
      "learning_rate": 1.5130975769482647e-05,
      "loss": 3.9493,
      "step": 10650
    },
    {
      "epoch": 7.0,
      "eval_loss": 4.0659379959106445,
      "eval_runtime": 1.0749,
      "eval_samples_per_second": 573.05,
      "eval_steps_per_second": 71.631,
      "step": 10689
    },
    {
      "epoch": 7.007203667321545,
      "grad_norm": 155.7048797607422,
      "learning_rate": 1.496725605762934e-05,
      "loss": 3.2063,
      "step": 10700
    },
    {
      "epoch": 7.039947609692207,
      "grad_norm": 110.94802856445312,
      "learning_rate": 1.4803536345776032e-05,
      "loss": 2.6252,
      "step": 10750
    },
    {
      "epoch": 7.072691552062868,
      "grad_norm": 92.63987731933594,
      "learning_rate": 1.4639816633922726e-05,
      "loss": 4.8334,
      "step": 10800
    },
    {
      "epoch": 7.10543549443353,
      "grad_norm": 106.62675476074219,
      "learning_rate": 1.4476096922069418e-05,
      "loss": 3.6659,
      "step": 10850
    },
    {
      "epoch": 7.138179436804191,
      "grad_norm": 314.4588317871094,
      "learning_rate": 1.431237721021611e-05,
      "loss": 3.4172,
      "step": 10900
    },
    {
      "epoch": 7.170923379174853,
      "grad_norm": 100.9709243774414,
      "learning_rate": 1.4148657498362802e-05,
      "loss": 3.1878,
      "step": 10950
    },
    {
      "epoch": 7.203667321545514,
      "grad_norm": 38.98956298828125,
      "learning_rate": 1.3984937786509497e-05,
      "loss": 3.037,
      "step": 11000
    },
    {
      "epoch": 7.2364112639161755,
      "grad_norm": 57.60247802734375,
      "learning_rate": 1.382121807465619e-05,
      "loss": 3.8004,
      "step": 11050
    },
    {
      "epoch": 7.269155206286837,
      "grad_norm": 215.00755310058594,
      "learning_rate": 1.3657498362802882e-05,
      "loss": 2.8588,
      "step": 11100
    },
    {
      "epoch": 7.3018991486574985,
      "grad_norm": 73.3993148803711,
      "learning_rate": 1.3493778650949576e-05,
      "loss": 3.6053,
      "step": 11150
    },
    {
      "epoch": 7.33464309102816,
      "grad_norm": 53.84614562988281,
      "learning_rate": 1.3330058939096269e-05,
      "loss": 3.4754,
      "step": 11200
    },
    {
      "epoch": 7.367387033398821,
      "grad_norm": 40.44706344604492,
      "learning_rate": 1.3166339227242961e-05,
      "loss": 3.686,
      "step": 11250
    },
    {
      "epoch": 7.400130975769483,
      "grad_norm": 43.575740814208984,
      "learning_rate": 1.3002619515389652e-05,
      "loss": 5.1341,
      "step": 11300
    },
    {
      "epoch": 7.432874918140144,
      "grad_norm": 745.2646484375,
      "learning_rate": 1.2838899803536347e-05,
      "loss": 3.6143,
      "step": 11350
    },
    {
      "epoch": 7.465618860510806,
      "grad_norm": 44.860687255859375,
      "learning_rate": 1.267518009168304e-05,
      "loss": 2.9207,
      "step": 11400
    },
    {
      "epoch": 7.498362802881467,
      "grad_norm": 122.52816009521484,
      "learning_rate": 1.2511460379829732e-05,
      "loss": 3.987,
      "step": 11450
    },
    {
      "epoch": 7.531106745252128,
      "grad_norm": 114.13648223876953,
      "learning_rate": 1.2347740667976425e-05,
      "loss": 3.6699,
      "step": 11500
    },
    {
      "epoch": 7.563850687622789,
      "grad_norm": 73.94300079345703,
      "learning_rate": 1.2184020956123119e-05,
      "loss": 3.2271,
      "step": 11550
    },
    {
      "epoch": 7.596594629993451,
      "grad_norm": 75.1636734008789,
      "learning_rate": 1.2020301244269811e-05,
      "loss": 2.2118,
      "step": 11600
    },
    {
      "epoch": 7.629338572364112,
      "grad_norm": 244.23101806640625,
      "learning_rate": 1.1856581532416504e-05,
      "loss": 2.631,
      "step": 11650
    },
    {
      "epoch": 7.662082514734774,
      "grad_norm": 137.4865264892578,
      "learning_rate": 1.1692861820563196e-05,
      "loss": 2.8273,
      "step": 11700
    },
    {
      "epoch": 7.694826457105435,
      "grad_norm": 41.447479248046875,
      "learning_rate": 1.152914210870989e-05,
      "loss": 4.2398,
      "step": 11750
    },
    {
      "epoch": 7.727570399476097,
      "grad_norm": 75.5925064086914,
      "learning_rate": 1.136542239685658e-05,
      "loss": 2.9342,
      "step": 11800
    },
    {
      "epoch": 7.760314341846758,
      "grad_norm": 26.108901977539062,
      "learning_rate": 1.1201702685003275e-05,
      "loss": 3.198,
      "step": 11850
    },
    {
      "epoch": 7.79305828421742,
      "grad_norm": 97.10934448242188,
      "learning_rate": 1.1037982973149969e-05,
      "loss": 3.2975,
      "step": 11900
    },
    {
      "epoch": 7.825802226588081,
      "grad_norm": 85.39867401123047,
      "learning_rate": 1.0874263261296661e-05,
      "loss": 3.7745,
      "step": 11950
    },
    {
      "epoch": 7.858546168958743,
      "grad_norm": 155.49411010742188,
      "learning_rate": 1.0710543549443354e-05,
      "loss": 3.4191,
      "step": 12000
    },
    {
      "epoch": 7.891290111329404,
      "grad_norm": 103.5076904296875,
      "learning_rate": 1.0546823837590046e-05,
      "loss": 2.3414,
      "step": 12050
    },
    {
      "epoch": 7.9240340537000655,
      "grad_norm": 50.58974838256836,
      "learning_rate": 1.038310412573674e-05,
      "loss": 5.223,
      "step": 12100
    },
    {
      "epoch": 7.956777996070727,
      "grad_norm": 56.58066940307617,
      "learning_rate": 1.021938441388343e-05,
      "loss": 2.4745,
      "step": 12150
    },
    {
      "epoch": 7.9895219384413885,
      "grad_norm": 54.28643035888672,
      "learning_rate": 1.0055664702030125e-05,
      "loss": 4.2019,
      "step": 12200
    },
    {
      "epoch": 8.0,
      "eval_loss": 4.056943893432617,
      "eval_runtime": 1.0864,
      "eval_samples_per_second": 567.019,
      "eval_steps_per_second": 70.877,
      "step": 12216
    },
    {
      "epoch": 8.02226588081205,
      "grad_norm": 86.64389038085938,
      "learning_rate": 9.891944990176817e-06,
      "loss": 4.0276,
      "step": 12250
    },
    {
      "epoch": 8.055009823182711,
      "grad_norm": 28.54755401611328,
      "learning_rate": 9.728225278323511e-06,
      "loss": 3.8932,
      "step": 12300
    },
    {
      "epoch": 8.087753765553373,
      "grad_norm": 88.62646484375,
      "learning_rate": 9.564505566470204e-06,
      "loss": 4.0795,
      "step": 12350
    },
    {
      "epoch": 8.120497707924034,
      "grad_norm": 31.25543975830078,
      "learning_rate": 9.400785854616896e-06,
      "loss": 3.6668,
      "step": 12400
    },
    {
      "epoch": 8.153241650294696,
      "grad_norm": 72.12651062011719,
      "learning_rate": 9.23706614276359e-06,
      "loss": 3.5009,
      "step": 12450
    },
    {
      "epoch": 8.185985592665357,
      "grad_norm": 92.17643737792969,
      "learning_rate": 9.073346430910281e-06,
      "loss": 2.0608,
      "step": 12500
    },
    {
      "epoch": 8.218729535036019,
      "grad_norm": 53.49370193481445,
      "learning_rate": 8.909626719056975e-06,
      "loss": 3.9926,
      "step": 12550
    },
    {
      "epoch": 8.25147347740668,
      "grad_norm": 270.0368347167969,
      "learning_rate": 8.745907007203667e-06,
      "loss": 4.8318,
      "step": 12600
    },
    {
      "epoch": 8.284217419777342,
      "grad_norm": 92.86091613769531,
      "learning_rate": 8.58218729535036e-06,
      "loss": 3.6872,
      "step": 12650
    },
    {
      "epoch": 8.316961362148003,
      "grad_norm": 74.3038330078125,
      "learning_rate": 8.418467583497052e-06,
      "loss": 2.5287,
      "step": 12700
    },
    {
      "epoch": 8.349705304518665,
      "grad_norm": 157.4263153076172,
      "learning_rate": 8.254747871643746e-06,
      "loss": 3.1725,
      "step": 12750
    },
    {
      "epoch": 8.382449246889326,
      "grad_norm": 429.94061279296875,
      "learning_rate": 8.09102815979044e-06,
      "loss": 3.8956,
      "step": 12800
    },
    {
      "epoch": 8.415193189259988,
      "grad_norm": 238.3023223876953,
      "learning_rate": 7.927308447937131e-06,
      "loss": 3.0019,
      "step": 12850
    },
    {
      "epoch": 8.447937131630649,
      "grad_norm": 164.0004119873047,
      "learning_rate": 7.763588736083825e-06,
      "loss": 3.4836,
      "step": 12900
    },
    {
      "epoch": 8.48068107400131,
      "grad_norm": 176.156005859375,
      "learning_rate": 7.599869024230518e-06,
      "loss": 3.6144,
      "step": 12950
    },
    {
      "epoch": 8.513425016371972,
      "grad_norm": 33.56964874267578,
      "learning_rate": 7.436149312377211e-06,
      "loss": 3.203,
      "step": 13000
    },
    {
      "epoch": 8.546168958742633,
      "grad_norm": 168.89341735839844,
      "learning_rate": 7.272429600523903e-06,
      "loss": 4.3801,
      "step": 13050
    },
    {
      "epoch": 8.578912901113295,
      "grad_norm": 19.99768829345703,
      "learning_rate": 7.1087098886705965e-06,
      "loss": 3.3446,
      "step": 13100
    },
    {
      "epoch": 8.611656843483955,
      "grad_norm": 183.8318328857422,
      "learning_rate": 6.94499017681729e-06,
      "loss": 3.736,
      "step": 13150
    },
    {
      "epoch": 8.644400785854616,
      "grad_norm": 106.18315887451172,
      "learning_rate": 6.781270464963982e-06,
      "loss": 3.126,
      "step": 13200
    },
    {
      "epoch": 8.677144728225278,
      "grad_norm": 76.9635009765625,
      "learning_rate": 6.617550753110675e-06,
      "loss": 3.0598,
      "step": 13250
    },
    {
      "epoch": 8.709888670595939,
      "grad_norm": 72.91344451904297,
      "learning_rate": 6.453831041257368e-06,
      "loss": 3.1504,
      "step": 13300
    },
    {
      "epoch": 8.7426326129666,
      "grad_norm": 103.6909408569336,
      "learning_rate": 6.290111329404061e-06,
      "loss": 2.8812,
      "step": 13350
    },
    {
      "epoch": 8.775376555337262,
      "grad_norm": 61.20075225830078,
      "learning_rate": 6.126391617550753e-06,
      "loss": 3.3449,
      "step": 13400
    },
    {
      "epoch": 8.808120497707923,
      "grad_norm": 77.80388641357422,
      "learning_rate": 5.962671905697447e-06,
      "loss": 3.3361,
      "step": 13450
    },
    {
      "epoch": 8.840864440078585,
      "grad_norm": 477.3540954589844,
      "learning_rate": 5.798952193844139e-06,
      "loss": 3.9246,
      "step": 13500
    },
    {
      "epoch": 8.873608382449246,
      "grad_norm": 88.59423065185547,
      "learning_rate": 5.635232481990832e-06,
      "loss": 2.3404,
      "step": 13550
    },
    {
      "epoch": 8.906352324819908,
      "grad_norm": 72.23297119140625,
      "learning_rate": 5.471512770137525e-06,
      "loss": 3.4902,
      "step": 13600
    },
    {
      "epoch": 8.93909626719057,
      "grad_norm": 138.77598571777344,
      "learning_rate": 5.307793058284218e-06,
      "loss": 2.623,
      "step": 13650
    },
    {
      "epoch": 8.97184020956123,
      "grad_norm": 34.20378494262695,
      "learning_rate": 5.14407334643091e-06,
      "loss": 2.8817,
      "step": 13700
    },
    {
      "epoch": 9.0,
      "eval_loss": 3.9255282878875732,
      "eval_runtime": 1.0957,
      "eval_samples_per_second": 562.202,
      "eval_steps_per_second": 70.275,
      "step": 13743
    },
    {
      "epoch": 9.004584151931892,
      "grad_norm": 147.71595764160156,
      "learning_rate": 4.980353634577603e-06,
      "loss": 4.2143,
      "step": 13750
    },
    {
      "epoch": 9.037328094302554,
      "grad_norm": 19.035913467407227,
      "learning_rate": 4.816633922724297e-06,
      "loss": 3.366,
      "step": 13800
    },
    {
      "epoch": 9.070072036673215,
      "grad_norm": 326.9568786621094,
      "learning_rate": 4.652914210870989e-06,
      "loss": 3.0015,
      "step": 13850
    },
    {
      "epoch": 9.102815979043877,
      "grad_norm": 132.99923706054688,
      "learning_rate": 4.4891944990176824e-06,
      "loss": 3.293,
      "step": 13900
    },
    {
      "epoch": 9.135559921414538,
      "grad_norm": 39.47212219238281,
      "learning_rate": 4.325474787164375e-06,
      "loss": 3.6002,
      "step": 13950
    },
    {
      "epoch": 9.1683038637852,
      "grad_norm": 139.41114807128906,
      "learning_rate": 4.161755075311067e-06,
      "loss": 3.4289,
      "step": 14000
    },
    {
      "epoch": 9.201047806155861,
      "grad_norm": 57.1919059753418,
      "learning_rate": 3.9980353634577605e-06,
      "loss": 2.689,
      "step": 14050
    },
    {
      "epoch": 9.233791748526523,
      "grad_norm": 129.63897705078125,
      "learning_rate": 3.834315651604453e-06,
      "loss": 3.4551,
      "step": 14100
    },
    {
      "epoch": 9.266535690897184,
      "grad_norm": 28.701339721679688,
      "learning_rate": 3.670595939751146e-06,
      "loss": 2.5204,
      "step": 14150
    },
    {
      "epoch": 9.299279633267846,
      "grad_norm": 50.78316116333008,
      "learning_rate": 3.5068762278978394e-06,
      "loss": 3.5441,
      "step": 14200
    },
    {
      "epoch": 9.332023575638507,
      "grad_norm": 98.32109832763672,
      "learning_rate": 3.343156516044532e-06,
      "loss": 4.2043,
      "step": 14250
    },
    {
      "epoch": 9.364767518009169,
      "grad_norm": 456.3210144042969,
      "learning_rate": 3.179436804191225e-06,
      "loss": 2.479,
      "step": 14300
    },
    {
      "epoch": 9.39751146037983,
      "grad_norm": 158.0494384765625,
      "learning_rate": 3.015717092337918e-06,
      "loss": 3.7381,
      "step": 14350
    },
    {
      "epoch": 9.430255402750491,
      "grad_norm": 139.67230224609375,
      "learning_rate": 2.8519973804846106e-06,
      "loss": 2.6836,
      "step": 14400
    },
    {
      "epoch": 9.462999345121153,
      "grad_norm": 61.728206634521484,
      "learning_rate": 2.6882776686313034e-06,
      "loss": 2.8699,
      "step": 14450
    },
    {
      "epoch": 9.495743287491814,
      "grad_norm": 113.42536926269531,
      "learning_rate": 2.524557956777996e-06,
      "loss": 2.664,
      "step": 14500
    },
    {
      "epoch": 9.528487229862476,
      "grad_norm": 92.35633087158203,
      "learning_rate": 2.360838244924689e-06,
      "loss": 2.4808,
      "step": 14550
    },
    {
      "epoch": 9.561231172233137,
      "grad_norm": 181.12362670898438,
      "learning_rate": 2.197118533071382e-06,
      "loss": 3.3771,
      "step": 14600
    },
    {
      "epoch": 9.593975114603799,
      "grad_norm": 106.36970520019531,
      "learning_rate": 2.0333988212180747e-06,
      "loss": 2.6436,
      "step": 14650
    },
    {
      "epoch": 9.62671905697446,
      "grad_norm": 148.68878173828125,
      "learning_rate": 1.8696791093647675e-06,
      "loss": 3.2832,
      "step": 14700
    },
    {
      "epoch": 9.659462999345122,
      "grad_norm": 73.05828094482422,
      "learning_rate": 1.7059593975114606e-06,
      "loss": 3.4598,
      "step": 14750
    },
    {
      "epoch": 9.692206941715783,
      "grad_norm": 135.7657012939453,
      "learning_rate": 1.5422396856581534e-06,
      "loss": 3.6359,
      "step": 14800
    },
    {
      "epoch": 9.724950884086445,
      "grad_norm": 175.6905975341797,
      "learning_rate": 1.3785199738048462e-06,
      "loss": 3.3879,
      "step": 14850
    },
    {
      "epoch": 9.757694826457106,
      "grad_norm": 52.24797821044922,
      "learning_rate": 1.214800261951539e-06,
      "loss": 3.7488,
      "step": 14900
    },
    {
      "epoch": 9.790438768827768,
      "grad_norm": 76.48513793945312,
      "learning_rate": 1.0510805500982318e-06,
      "loss": 2.8161,
      "step": 14950
    },
    {
      "epoch": 9.82318271119843,
      "grad_norm": 103.91305541992188,
      "learning_rate": 8.873608382449248e-07,
      "loss": 3.1495,
      "step": 15000
    },
    {
      "epoch": 9.855926653569089,
      "grad_norm": 107.50080871582031,
      "learning_rate": 7.236411263916176e-07,
      "loss": 2.4836,
      "step": 15050
    },
    {
      "epoch": 9.88867059593975,
      "grad_norm": 232.45809936523438,
      "learning_rate": 5.599214145383104e-07,
      "loss": 2.4334,
      "step": 15100
    },
    {
      "epoch": 9.921414538310412,
      "grad_norm": 32.784576416015625,
      "learning_rate": 3.9620170268500327e-07,
      "loss": 2.2036,
      "step": 15150
    },
    {
      "epoch": 9.954158480681073,
      "grad_norm": 205.619873046875,
      "learning_rate": 2.3248199083169614e-07,
      "loss": 4.3751,
      "step": 15200
    },
    {
      "epoch": 9.986902423051735,
      "grad_norm": 29.32412338256836,
      "learning_rate": 6.876227897838901e-08,
      "loss": 4.1405,
      "step": 15250
    },
    {
      "epoch": 10.0,
      "eval_loss": 3.936553478240967,
      "eval_runtime": 1.0849,
      "eval_samples_per_second": 567.799,
      "eval_steps_per_second": 70.975,
      "step": 15270
    }
  ],
  "logging_steps": 50,
  "max_steps": 15270,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 1
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6602933619037800.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
